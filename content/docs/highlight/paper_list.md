---
title: "Paper List"
description: "A curated collection of influential research papers in robotics, computer vision, and machine learning"
summary: "Featured papers with visual summaries, arXiv links, and brief descriptions"
date: 2023-09-07T16:13:18+02:00
lastmod: 2023-09-07T16:13:18+02:00
draft: false
weight: 910
toc: true
seo:
  title: "Research Paper List" # custom title (optional)
  description: "Explore our curated collection of influential research papers in robotics, computer vision, and machine learning" # custom description (recommended)
  canonical: "" # custom canonical URL (optional)
  noindex: false # false (default) or true
---

# 具身亮点论文

## Humanoid Locomotion/Imitation Learning

**ASAP: Aligning Simulation and Real-World Physics for Learning Agile Humanoid Whole-Body Skills**

{{< figure src="images/papers/ASAP.png" alt="ASAP" caption="ASAP teaser" width="300" >}}

**arXiv Link:** [ASAP Paper](https://arxiv.org/pdf/2502.01143)

**Description:** 人形机器人对高敏捷人类行为的模仿学习。

---

**TWIST: Teleoperated Whole-Body Imitation System**

{{< figure src="images/papers/TWIST.png" alt="TWIST" caption="TWIST Teaser" width="300" >}}

**arXiv Link:** [TWIST Paper](https://arxiv.org/pdf/2505.02833)

**Description:** 人形机器人全身遥操作。

---

## Manipulation and Vision Language Model

### OpenVLA系列工作

**OpenVLA: An Open-Source Vision-Language-Action Model**

{{< figure src="images/papers/OpenVLA.png" alt="openvla" caption="OpenVLA teaser" width="300" >}}

**arXiv Link:** [Open-VLA Paper](https://arxiv.org/pdf/2406.09246)

**Description:** 具身操作VLA foundation model

---

**Fine-Tuning Vision-Language-Action Models: Optimizing Speed and Success**

{{< figure src="images/papers/openvla-oft.png" alt="oft" caption="OpenVLA-OFT teaser" width="300" >}}

**arXiv Link:** [Open-VLA-OFT Paper](https://arxiv.org/pdf/2502.19645)

**Description:** 具身操作VLA foundation model

---

**RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation**

{{< figure src="images/papers/RDT.png" alt="RDT" caption="RDT teaser" width="300" >}}

**arXiv Link:** [RDT Paper](https://arxiv.org/pdf/2410.07864)

**Description:** 双臂协同操作foundation model。

---

### TikTok GR系列工作

**UNLEASHING LARGE-SCALE VIDEO GENERATIVE PRE-TRAINING FOR VISUAL ROBOT MANIPULATION**

{{< figure src="images/papers/GR1.png" alt="gr1" caption="GR-1 teaser" width="300" >}}

**arXiv Link:** [GR-1 Paper](https://arxiv.org/pdf/2312.13139)

---

**GR-2: A Generative Video-Language-Action Model with Web-Scale Knowledge for Robot Manipulation**

{{< figure src="images/papers/GR2.png" alt="gr2" caption="GR-2 teaser" width="300" >}}

**arXiv Link:** [GR-2 Paper](https://arxiv.org/pdf/2410.06158)

---

**GR-3 Technical Report**

{{< figure src="images/papers/GR3.png" alt="gr3" caption="GR-3 teaser" width="300" >}}

**arXiv Link:** [GR-3 Paper](https://arxiv.org/html/2507.15493v1)

**Description:** 字节跳动提出的基于大规模视频预训练模型

---
### π系列工作

**π0: A Vision-Language-Action Flow Model for General Robot Control**

{{< figure src="images/papers/pi0.png" alt="pi0" caption="pi0 teaser" width="300" >}}

**arXiv Link:** [π0 Paper](https://arxiv.org/pdf/2410.24164v1)

**Description:** PI系列VLA关键工作

---

**π0.5: a Vision-Language-Action Model with Open-World Generalization**

{{< figure src="images/papers/pi05.png" alt="pi0.5" caption="pi0.5 teaser" width="300" >}}

**arXiv Link:** [π0.5 Paper](https://arxiv.org/pdf/2410.24164v1)

**Description:** PI系列VLA关键工作

---


## 仿真平台和基准

**RoboVerse: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning**

{{< figure src="images/papers/RoboVerse.png" alt="rbv" caption="RoboVerse teaser" width="300" >}}

**arXiv Link:** [RoboVerse Paper](https://arxiv.org/pdf/2504.18904)

**Description:** 最全面的仿真平台集成。

*Last updated: Aug 2025*

*Keep Updated at [VapourX-Highlight-Paper](https://acondaway.github.io/vapourx/docs/highlight/paper-list/)*

